{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this step, we conduct data preprocessing tasks such as imputing missing data and feature engineering. Utilize the `Data_Preprocessing.ipynb` notebook to impute missing values and engineer features that might enhance the predictive model's performance.\n",
    "\n",
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import utils as ui\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Imputation Methods\n",
    "\n",
    "### 1. PPCA Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPCA_Imputation(df,filename,split):\n",
    "\n",
    "    # Replace NaN values with the mean for each column\n",
    "    df_filled = df.fillna(df.mean())\n",
    "\n",
    "    # Perform PPCA\n",
    "    num_latent_variables = 2  # You can adjust this based on your analysis\n",
    "    ppca_model = FactorAnalysis(n_components=num_latent_variables)\n",
    "    df_transformed = ppca_model.fit_transform(df_filled)\n",
    "\n",
    "    # Transform the imputed data back to the original space\n",
    "    df_imputed = pd.DataFrame(np.dot(df_transformed, ppca_model.components_) + ppca_model.mean_, columns=df.columns, index=df.index)\n",
    "\n",
    "    df_result=df.combine_first(df_imputed)\n",
    "\n",
    "    # Specifying the file path where you want to save the CSV file\n",
    "    directory=\"./Datasets/With Imputation/PPCA Imputation\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    csv_file_path = directory+f'/{filename}_{split}_PPCA.csv'\n",
    "\n",
    "    # Writing the DataFrame to a CSV file\n",
    "    df_result.to_csv(csv_file_path)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def et_reg(df,filename,split):   \n",
    " \n",
    "    imputer = IterativeImputer(estimator=ExtraTreesRegressor(n_jobs=1),max_iter=10)\n",
    "    df_et=imputer.fit_transform(df) # Fitted the imputer to the data and transformed it\n",
    "    df_et =pd.DataFrame(df_et, columns=df.columns) \n",
    "    df_et.index=df.index\n",
    "    \n",
    "    # Specifying the file path where you want to save the CSV file\n",
    "    directory=\"./Datasets/With Imputation/ET_Regressor Imputation\"\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    csv_file_path = directory+f'/{filename}_{split}_ET.csv'\n",
    "    \n",
    "    # Writing the DataFrame to a CSV file\n",
    "    df_et.to_csv(csv_file_path)\n",
    "\n",
    "    return df_et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Imputed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(filepath,filename,imputation=\"PPCA\"):\n",
    "\n",
    "    df = ui.load_csv(filepath)\n",
    "\n",
    "    df_train, df_test = ui.train_test_split(df)\n",
    "\n",
    "    if imputation==\"PPCA\":\n",
    "        df_train=PPCA_Imputation(df_train,filename,split=\"train\")\n",
    "        df_test=PPCA_Imputation(df_test,filename,split=\"test\")\n",
    "    elif imputation==\"ET_Regressor\":\n",
    "        df_train=et_reg(df_train,filename,split=\"train\")\n",
    "        df_test=et_reg(df_test,filename,split=\"test\")\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test=preprocess_csv(\".\\Datasets\\Without Imputation\\Final_Dataset_Ghaziabad.csv\",\"Ghaziabad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
